# -1
介绍决策的基本实现方法，附带数据
可以把决策树理解为选择对某个特征的最好的划分，然后一层层的划分，最好的特征度量是通过熵，背后的含义应该是分而治之。
即把好处理的放到一侧，这里好处理是因为类别一致
就好像当初把中华民族划分成56个民族一样。然后分而治之，但这也有弊端，过分强调不同会导致内斗，和内耗。比如现在少数民族对汉族的敌视（帝王之术）
而最好能度量划分效果的就是熵，但我觉得还不算太完全，其实应该找个某一特征分类出来的最大熵，而不是某一特征分类出来所有熵。
但是若是刻画特征就是不同分类下的熵（暂时存疑）
那么最后的损失函数怎么做，即怎么用数学刻画我的想法，最理想的是每个叶节点都只包含一个类，包含越多说明划分越不好，那么自然想到用熵的概念
我们知道熵其实是一中期望的概念，因此真正的还有有Nt（即在t节点时存在Nt个）但是我们肯定不希望叶节点太多，因为太多就意味着对训练数据刻画的比较严重。
因此在最后价格L1正则把保证叶节点不要太多。
![image](https://github.com/chenglu66/decision-TREE/决策树.png)
所以整体思路就是。
读取数据，并做预处理。决策树是概率的概念，所以不要什么归一化之类的。下面划分怎么化了，即我这种划分怎么和目标函数联系起来。
假如把每次划分都是熵减小最大的方向，那么最后结果也应该是熵最小的。当然划分出来的数目是比较麻烦的。所以如果能有概率去衡量也行。
因此ID3算法把划分和目标函数联系起来了。
即假设最后都是同一类时这两种方法等价（ID3算法相当于做的是极大释然估计）
具体流程：
找到最佳特征：
这个最佳特征是有概率的概念，即假设每次都是叶节点，那么叶节点的数目/总的数目（叶节点的熵）就是我们的目标函数。
所以是全部的和也就是可以理解的了。
决策树的另一种应用，决策数可以对数据进行分类，但是如果对指标稍微改下，也可以做回归，
分类之后是不是意味着可以分段建模。即局部建模。至于怎么分类就要看你的指标怎么去做。怎么去衡量接近程度，欧式距离到哪点的欧式距离，要不到均值的
欧式距离吧，那这些是不是就是方差的和了。所以损失函数找到了，下面就是实施。当然叶节点不能太多，还是L1正则吧。（这个等同于最小二乘回归，即把叶节点想象
成一个固定的值。
思路：找一个特征，做二元切分计算其最小值。这样，二元切分比较讲究。最保险方法就是把你每个特征的取值都做次切分计算下。麻烦是麻烦点，但准确
那切分到什么时候，误差比较小，或者是切分点的数目比较少了。
树的剪纸：可以看出回归树的出口条件可以改变分类点的数目。但是效果好像不太好，因此还要做剪纸，就意味着我取得节点太多了，因此有必要去点几个。
那么思路也就比较简单了，找两个相邻的叶节点，如果合并的误差小于合并前的就合并，这样自然就减少了叶节点的数目。
具体流程：
模型树：与最小二乘回归树不同的是模型树叶节点是个模型，不是一个值。那么指标还是最小二乘，就是减去值不是一个固定值而是一个模型获得值。
至于模型的选择，已经知道了输入数据和输出数据，指标又是最小二乘，那么直接解吧，也不用啥梯度下降法。
除了最小二乘这一种指标，还有一种可以刻画的方法，即把点看成概率，那么基尼系数可以表示接近程度。越小越接近，越大差距也就越大。
计算公式为sum（p（1-p））也可以理解因为越是都可能越是不稳定。小概率事件真实世界中基本不会发生。
总结下：模型的是弄清目标，然后用一些数学指标来衡量，最后是设计算法来执行这些指标，这里的指标也可以理解为误差函数。



