# -1
介绍决策的基本实现方法，附带数据
可以把决策树理解为选择对某个特征的最好的划分，然后一层层的划分，最好的特征度量是通过熵，背后的含义应该是分而治之。
即把好处理的放到一侧，这里好处理是因为类别一致
就好像当初把中华民族划分成56个民族一样。然后分而治之，但这也有弊端，过分强调不同会导致内斗，和内耗。比如现在少数民族对汉族的敌视（帝王之术）
而最好能度量划分效果的就是熵，但我觉得还不算太完全，其实应该找个某一特征分类出来的最大熵，而不是某一特征分类出来所有熵。
但是若是刻画特征就是不同分类下的熵（暂时存疑）
那么最后的损失函数怎么做，即怎么用数学刻画我的想法，最理想的是每个叶节点都只包含一个类，包含越多说明划分越不好，那么自然想到用熵的概念
我们知道熵其实是一中期望的概念，因此真正的还有有Nt（即在t节点时存在Nt个）但是我们肯定不希望叶节点太多，因为太多就意味着对训练数据刻画的比较严重。
因此在最后价格L1正则把保证叶节点不要太多。
所以整体思路就是。
读取数据，并做预处理。决策树是概率的概念，所以不要什么归一化之类的。下面划分怎么化了，即我这种划分怎么和目标函数联系起来。
假如把每次划分都是熵减小最大的方向，那么最后结果也应该是熵最小的。当然划分出来的数目是比较麻烦的。所以如果能有概率去衡量也行。
因此ID3算法把划分和目标函数联系起来了。
即假设最后都是同一类时这两种方法等价（ID3算法相当于做的是极大释然估计）
具体流程：
找到最佳特征：
这个最佳特征是有概率的概念，即假设每次都是叶节点，那么叶节点的数目/总的数目*（叶节点的熵）就是我们的目标函数。
所以是全部的和也就是可以理解的了。
